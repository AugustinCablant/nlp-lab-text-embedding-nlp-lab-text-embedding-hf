{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Ce notebook utilise des données fournies par le [Musée Guimet](https://www.guimet.fr/fr) dans le cadre du [projet HikarIA](https://www.guimet.fr/fr/actualites-du-musee/le-projet-hikaria-laureat-de-france-2030), un projet de recherche visant à développer un système de cataloguage automatique d'une collection d'albums de photographies japonaises du XIXème. \n",
    "\n",
    "Ce premier TP utilise un ensemble de tags (`tags.txt`) utilisés pour décrire les photographies.\n",
    "\n",
    "<img src=\"https://storage.teklia.com/shared/deepnlp-labs/images/16-519781.jpg\" width=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings statiques Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "Dans cette section, nous allons explorer un modèle de plongement de mots pré-entraîné de type Word2Vec.\n",
    "Nous allons utiliser un modèle Word2Vec pré-entraîné sur le corpus français Wac. \n",
    "Ce modèle a été entraîné sur un corpus de 1 milliard de mots en français. \n",
    "\n",
    "Cet embedding est disponble sous 2 formats:\n",
    "- un format texte qui permet d'explorer facilement le modèle :\n",
    "    - [frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt)\n",
    "- un format binaire qui peut être chargé avec la librairie Gensim : \n",
    "    - [frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin)\n",
    "\n",
    "### Embedding de mot\n",
    "\n",
    "Téléchargez le fichier texte sur votre machine pour l'analyser.\n",
    "\n",
    "#### Question : \n",
    ">* Donner la taille des fichiers d'embeddings\n",
    ">* En explorant le contenu du fichier d'embedding au format texte, donner le nombre de mots pour lesquels ce modèle  fournit des embeddings et la taille de l'embedding pour chacun des mots\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarité de mots\n",
    "\n",
    "Nous allons maintenant utiliser la librairie [Gensim](https://radimrehurek.com/gensim/) pour charger le modèle Word2Vec et l'utilser. \n",
    "\n",
    "#### Question : \n",
    ">* Modifier le code suivant pour charger le fichier modèle Word2Vec au format binaire\n",
    ">* Choisissez quelques exemples dans le fichier tags.csv et utilisez la fonction [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) pour trouver les mots les plus proches selon le modèle\n",
    ">* Afin de deviner le sens des mots \"yokohama\", \"kanto\" et \"shamisen\", cherchez leurs plus proches voisins. Expliquez les résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the French word embeddings\n",
    "!wget \"https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"-O frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## YOUR CODE HERE\n",
    "embedding_file =\"\"\n",
    "model = KeyedVectors.load_word2vec_format(embedding_file, binary=True, unicode_errors=\"ignore\")\n",
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmétique sémantique\n",
    "\n",
    "Une des propriétés les plus originales des embeddings Word2Vec est que les relations sémantiques entre les vecteurs peuvent être modélisées par des opérations arithmétiques. Etant données les vecteurs repésentant les mots `roi`, `homme` et `femme`, il est possible de calculer le vecteur `v` comme :  \n",
    "\n",
    "`v = vecteur(roi)-vecteur(homme)+vecteur(femme)`\n",
    "\n",
    "Cette opération correspond à la relation sémantique suivante : *Le roi est à l'homme ce que la reine est à la femme* ce qui se traduit par l'arithmétique suivante : *le concept de roi, moins le concept d'homme  plus le concept de femme donne le concept de reine*.\n",
    "\n",
    "En effet, en cherchant dans l'embedding, le mot dont le vecteur le plus proche est `v`, on trouve `reine`\n",
    "\n",
    "#### Question : \n",
    ">* en utilisant la fonction [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) en spécifiant les arguments `positive` pour les vecteurs à additionner et `negative` pour les vecteurs à soustraire, vérifier la relation *le concept de roi, moins le concept d'homme  plus le concept de femme donne le concept de reine*\n",
    ">* Avec la même méthode, trouvez XXX dans les relations sémantiques suivantes\n",
    ">   * *Paris est à la France ce que XXX est au Japon*\n",
    ">   * *Chevalier est à la France ce qui XXX est au Japon*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding d'une séquence de mots\n",
    "Les embeddings Word2Vec permettent de représenter des mots sous forme vectorielle. Pour représenter des séquences de mots (expressions, phrases, documents), il faut  donc combiner les vecteurs de chacun des mots pour obtenir une représentation vectorielle de la séquence. La technique la plus simple consiste à prendre la moyenne des vecteurs des mots de la séquence. Cette approche a cependant l'inconvient de ne pas prendre en compte l'ordre des mots et d'être sensible aux points abérants (outliers).\n",
    "\n",
    "#### Question : \n",
    ">* en utilisant la fonction [get_mean_vector](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.get_mean_vector) calculer les représentations vectorielles des phases suivantes : \n",
    ">   * *le cerisier est en fleurs*\n",
    ">   * *le jardin est fleuri*\n",
    ">   * *la femme joue de la musique*\n",
    ">* en utilisant la fonction [cosine_similarities](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.cosine_similarities), calculer les similarités entre la première phrase et les deux autres. En déduire quelles phrases sont les plus proches selon le modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # YOUR CODE HERE\n",
    "phrase1 = \"le cerisier est en fleurs\"\n",
    "phrase2 = \"le jardin est fleuri\"\n",
    "phrase3 = \"la femme joue de la musique\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation graphique\n",
    "\n",
    "Les embeddings permettent de représenter les mots dans un espace en grande dimension (200, 756, 1500, etc). L'utilisation d'un espace en grande dimension est utile pour réprésenter les différents sens des mots et leurs caractéristiques grammaticales, mais elle rend la visualisation complexe. Afin de visualiser les positions relatives des mots, il faut réduire la dimension des embedding pour les afficher en dimension 2 par exemple. Il faut que la réduction de dimension respecte la répartition initiale des points dans l'espace pour que la visualisation en 2D garde un sens. Différentes techniques de [réduction de dimension](https://cedric.cnam.fr/vertigo/Cours/ml/coursReductionDimension.html) sont utilisables (ACP, LLE, t-SNE). Nous allons utiliser t-SNE.\n",
    "\n",
    "#### Question : \n",
    ">* Explorer la visalisation t-SNE, noter quelques clusters de mots sémantiquement similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tags file\n",
    "!wget https://storage.teklia.com/shared/deepnlp-labs/tags.txt -O tags.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the tags\n",
    "tags_file = 'tags.txt'\n",
    "tags = [tag.strip() for tag in open(tags_file, 'r')]\n",
    "\n",
    "# Take a subset of the tags of 200 words\n",
    "tag_subset = tags[:200]\n",
    "\n",
    "# Compute the vectors for the tags\n",
    "vectors = np.array([model[tag] for tag in tag_subset])\n",
    "\n",
    "# reduce the dimensionality of the vectors with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_results = tsne.fit_transform(vectors)\n",
    "\n",
    "# Prepare the data for visualisation\n",
    "df = pd.DataFrame(tsne_results, columns=['Dimension 1', 'Dimension 2'])\n",
    "df['Tag'] = tag_subset\n",
    "# Visualise the tags\n",
    "fig = px.scatter(df, x='Dimension 1', y='Dimension 2', text='Tag', title='Visualisation des tags avec t-SNE')\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dynamique BERT\n",
    "### Le modèle Camembert\n",
    "Dans cette section, nous allons étudier le modèle BERT et particulièrement un modèle BERT entrainé sur un corpus français, le modèle [Camembert](https://camembert-model.fr/). \n",
    "\n",
    "Pour utiliser le modèle, nous allons utiliser la librairie `transformers` de [HuggingFace](https://huggingface.co/docs/transformers/index). Cette librairie permet d'utiliser très facilement un très grand nombre de modèles avec les mêmes interfaces. \n",
    "\n",
    "L'utilisation du modèle nécessite le chargement de deux composants : \n",
    "- le modèle lui-même `CamembertForMaskedLM.from_pretrained('camembert-base')`\n",
    "- le tokenizer, qui permet de découper les textes en tokens utilisables par le modèle pour faire des prédictions: `AutoTokenizer.from_pretrained('camembert-base')`\n",
    "\n",
    "Chargeons ces deux composants et affichons la descriptions du modèle : \n",
    "- nombre de répétitions du bloc d'encodeur\n",
    "- la définition du premier bloc, qui contient l'embedding initial des tokens\n",
    "\n",
    "#### Question : \n",
    ">* en lisant la description du premier bloc d'encodeur, donner le nombre de tokens modélisés par le ce modèle et la taille des embeddings.\n",
    ">* vérifier que vous retouvez les informations données dans la [documentation sur HuggingFace](https://huggingface.co/docs/transformers/en/model_doc/camembert#transformers.CamembertConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Import the model and tokenizer\n",
    "camembert = CamembertForMaskedLM.from_pretrained('camembert-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "# Print a description of the model\n",
    "print(f\"Ce modèle Camembert est composé de {len(camembert.roberta.encoder.layer)} blocs d'encodeur\\n\")\n",
    "print (\"Description de l'embedding initial :\")\n",
    "print (camembert.roberta.embeddings)\n",
    "print(\"Composition de la première couche :\")\n",
    "print (camembert.roberta.encoder.layer[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction en masquage\n",
    "\n",
    "Le modèle est entrainé, sur une grande quantité de texte en français, à prédire des mots masqués dans la phrase en fonction du contexte. Le modèle doit donc apprendre le sens et la séquence des mots pour prédire les mots les plus probables étant donné un contexte. Une fois entrainé, le modèle peut donc prédire des mots masqués dans une phrase.\n",
    "\n",
    "La première étape consiste à découper la phrase en utilisant les tokens modélisés par le modèle. La taille maximale d'une phrase que le modèle peut traiter est fixe. Les phrases sont donc complétées avec des token de `padding` pour atteindre cette taille.\n",
    "\n",
    "#### Question : \n",
    ">* Executer la cellule suivante et donner :\n",
    ">   * le nombre maximal de tokens dans une phrase traitée par le modèle\n",
    ">   * le nombre de tokens utilisés pour encoder la phrase `Les cerisiers sont en`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the sentence to complete\n",
    "batch_sentences = [\n",
    "    \"Les cerisiers sont en <mask>\",\n",
    "    \"Le samourail sort son <mask>\",\n",
    "    \"Tokyo est la capitale du <mask>\",\n",
    "]\n",
    "# Tokenize the sentences\n",
    "tokenizer_output = tokenizer(\n",
    "    batch_sentences,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Print the tokenized sentences\n",
    "print (\"nombres de tokens :\")\n",
    "print (len(tokenizer_output[\"input_ids\"][0]))\n",
    "print (\"identifiants des tokens de la première phrase :\")\n",
    "print (tokenizer_output[\"input_ids\"][0])\n",
    "print (\"tokens de la première phrase :\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer_output[\"input_ids\"][0])\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite utiliser le modèle pour prédire les mots masqués. La prédiction est réalisée en appelant le modèle (fonction `__call__` en python). Cet appel prend la phrase tokenisée en entrée et retourne une structure qui contient : \n",
    "- la loss (None, car nous utilsons `torch.no_grad()`, pas besoin de calculer la loss pour une inférence)\n",
    "- les logits, c'est à dire les scores de chaque token possible à chaque position\n",
    "- les couches cachées du modèle (si on demande `output_hidden_states=True`)\n",
    "\n",
    "#### Question : \n",
    ">* Executer la cellule suivante et expliquer à quoi correspondent les 3 dimensions de la variable `logit`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model_output = camembert(**tokenizer_output)\n",
    "pprint(model_output)\n",
    "pprint(model_output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `logits`contient les scores de chaque token possible à chaque position dans la phrase. Ces scores ne sont pas normalisés, pour les transformer en probabilités (entre 0 et 1), nous utilisons une [fonction softmax](https://en.wikipedia.org/wiki/Softmax_function). Puis la fonction  `topk` de la [librairie torch](https://pytorch.org/docs/stable/generated/torch.topk.html) nous donne les index des 5 plus grandes probabilités. Il reste ensuite à transformer les index en token avec la fonction `convert_ids_to_tokens`.\n",
    "\n",
    "#### Question : \n",
    ">* Modifier le code suivant pour donner les 5 mots les plus probables pour chacune des phrases à compléter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the logits for the masked token in the first sentence\n",
    "masked_index = 6\n",
    "sentence_logits = model_output.logits[0, masked_index]\n",
    "# get the probabilities by applying the softmax function\n",
    "probs = torch.nn.functional.softmax(sentence_logits, dim=0)\n",
    "# get the top 3 predictions\n",
    "values, predictions = torch.topk(probs, 3)\n",
    "# print the top 3 predictions\n",
    "for value, prediction in zip(values, predictions):\n",
    "    token = tokenizer.convert_ids_to_tokens([prediction])[0]\n",
    "    print(f\"{token}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
